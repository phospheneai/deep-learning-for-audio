{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWOirozFtUiS",
    "outputId": "918922d1-7ec1-47a9-a12f-c631c463c9cb"
   },
   "outputs": [],
   "source": [
    "!pip install soundata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AgIgIuBtWVz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlaUqztuuxQY"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tRjob7-tbec",
    "outputId": "12baefaa-d0f0-4d81-d103-fa5b60b4088c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.61GB [08:08, 12.3MB/s]                            \n",
      "1.15MB [00:01, 672kB/s]                           \n",
      "100%|██████████| 1/1 [00:00<00:00, 397.19it/s]\n",
      "100%|██████████| 8732/8732 [00:52<00:00, 167.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'metadata': {}, 'clips': {}}, {'metadata': {}, 'clips': {}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = soundata.initialize('urbansound8k', data_home='/content/data')\n",
    "dataset.download()  # download the dataset\n",
    "dataset.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvJLzqtP2J7l"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3bXqFaZt9A2"
   },
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "  def __init__(self, metadata_file, audio_dir, transform=None, max_length = 128):\n",
    "    self.metadata = pd.read_csv(metadata_file)\n",
    "    self.audio_dir = audio_dir\n",
    "    self.transform = transform\n",
    "    self.max_length = max_length\n",
    "\n",
    "    self.label_encoder = LabelEncoder()\n",
    "    self.metadata['encoded_label'] = self.label_encoder.fit_transform(self.metadata['class'])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.metadata)\n",
    "\n",
    "  def pad_or_truncate(self, mel_spec):\n",
    "    if mel_spec.shape[1] > self.max_length:\n",
    "      mel_spec = mel_spec[:, :self.max_length]\n",
    "\n",
    "    else:\n",
    "      pad_width = self.max_length - mel_spec.shape[1]\n",
    "      mel_spec = np.pad(mel_spec, pad_width=((0,0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mel_spec\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.metadata.iloc[idx]\n",
    "    audio_path = os.path.join(self.audio_dir, f\"fold{str(row['fold'])}\", row['slice_file_name'])\n",
    "    label = row['encoded_label']\n",
    "\n",
    "    try:\n",
    "      signal, sr = librosa.load(audio_path, sr=22050)\n",
    "      mel_spec = librosa.feature.melspectrogram(\n",
    "          y=signal,\n",
    "          sr=sr,\n",
    "          n_mels=128,\n",
    "          hop_length=512\n",
    "        )\n",
    "      mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "      mel_spec_db = self.pad_or_truncate(mel_spec_db)\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error processing {audio_path}: {e}\")\n",
    "      return None, None\n",
    "\n",
    "    if self.transform:\n",
    "      mel_spec_db = self.transform(mel_spec_db)\n",
    "    mel_spec_db = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)\n",
    "    return mel_spec_db, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYdHQD9Stg1f"
   },
   "outputs": [],
   "source": [
    "metadata_path = '/content/data/metadata/UrbanSound8K.csv'\n",
    "audio_path = '/content/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeJUmPj4yNUB"
   },
   "outputs": [],
   "source": [
    "dataset = UrbanSoundDataset(metadata_path, audio_path)\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the dataset for training\n",
    "val_size = len(dataset) - train_size  # Remaining 20% for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4lVaT4zyZSR",
    "outputId": "76c943ee-a905-42d6-dd0c-4cce849c7fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 128, 128])\n",
      "Label batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "for features, labels in train_dataloader:\n",
    "    print(f\"Feature batch shape: {features.shape}\")\n",
    "    print(f\"Label batch shape: {labels.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZtmPrtlyfLx"
   },
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # Input: (1, 128, 128)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: (64, 64, 64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Halves each dimension\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 256)  # Adjust dimensions based on input size\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Conv -> BN -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Final output (logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruSXGMp9h2XY"
   },
   "outputs": [],
   "source": [
    "class ResNet18AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18AudioClassifier, self).__init__()\n",
    "\n",
    "        # Load ResNet-18 without pretrained weights\n",
    "        self.resnet18 = models.resnet18(pretrained=False)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 1 channel input (grayscale)\n",
    "        self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Modify the fully connected layer to match the number of output classes\n",
    "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmZtyz0j1Reg",
    "outputId": "660f6dd9-d66b-42d5-dae1-e0d39179042f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(dataset.label_encoder.classes_)\n",
    "model = ResNet18AudioClassifier(num_classes).to(device)\n",
    "\n",
    "print(model(features.to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DKGY5UV1TeE"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "an8VIfKv14CO"
   },
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNzN8bC22Y4C",
    "outputId": "2c6df7c5-2898-4cd9-fdd4-11e820c9c243"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/15:   0%|          | 0/219 [00:00<?, ?batch/s]/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "Epoch 1/15:  19%|█▊        | 41/219 [00:32<02:46,  1.07batch/s, accuracy=0.327, loss=1.59]/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "Epoch 1/15:  30%|███       | 66/219 [00:50<01:47,  1.42batch/s, accuracy=0.384, loss=1.3]/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "Epoch 1/15: 100%|██████████| 219/219 [02:46<00:00,  1.32batch/s, accuracy=0.553, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.2737, Accuracy: 55.29%, Time: 166.18s\n",
      "Validation Loss: 1.0881, Validation Accuracy: 63.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 219/219 [02:40<00:00,  1.37batch/s, accuracy=0.769, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 0.6879, Accuracy: 76.92%, Time: 160.28s\n",
      "Validation Loss: 1.2664, Validation Accuracy: 62.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 219/219 [02:40<00:00,  1.36batch/s, accuracy=0.827, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Loss: 0.5041, Accuracy: 82.69%, Time: 160.78s\n",
      "Validation Loss: 1.4200, Validation Accuracy: 61.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 219/219 [02:32<00:00,  1.43batch/s, accuracy=0.879, loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Loss: 0.3729, Accuracy: 87.89%, Time: 152.88s\n",
      "Validation Loss: 0.9498, Validation Accuracy: 74.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 219/219 [02:33<00:00,  1.43batch/s, accuracy=0.905, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Loss: 0.3024, Accuracy: 90.51%, Time: 153.29s\n",
      "Validation Loss: 0.5584, Validation Accuracy: 81.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 219/219 [02:32<00:00,  1.44batch/s, accuracy=0.92, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Loss: 0.2490, Accuracy: 92.04%, Time: 152.11s\n",
      "Validation Loss: 0.4950, Validation Accuracy: 84.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 219/219 [02:32<00:00,  1.44batch/s, accuracy=0.939, loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Loss: 0.1908, Accuracy: 93.86%, Time: 152.19s\n",
      "Validation Loss: 0.5066, Validation Accuracy: 84.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 219/219 [02:34<00:00,  1.42batch/s, accuracy=0.943, loss=0.0817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Loss: 0.1601, Accuracy: 94.29%, Time: 154.08s\n",
      "Validation Loss: 0.9699, Validation Accuracy: 74.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 219/219 [02:33<00:00,  1.43batch/s, accuracy=0.948, loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Loss: 0.1436, Accuracy: 94.80%, Time: 153.51s\n",
      "Validation Loss: 0.3541, Validation Accuracy: 88.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 219/219 [02:33<00:00,  1.42batch/s, accuracy=0.954, loss=0.0226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Loss: 0.1200, Accuracy: 95.45%, Time: 153.72s\n",
      "Validation Loss: 0.4482, Validation Accuracy: 87.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 219/219 [02:35<00:00,  1.41batch/s, accuracy=0.97, loss=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Loss: 0.0866, Accuracy: 96.95%, Time: 155.06s\n",
      "Validation Loss: 0.5398, Validation Accuracy: 86.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 219/219 [02:31<00:00,  1.44batch/s, accuracy=0.96, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Loss: 0.1048, Accuracy: 96.03%, Time: 151.58s\n",
      "Validation Loss: 0.9068, Validation Accuracy: 75.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 219/219 [02:33<00:00,  1.43batch/s, accuracy=0.966, loss=0.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Loss: 0.0954, Accuracy: 96.61%, Time: 153.02s\n",
      "Validation Loss: 0.4506, Validation Accuracy: 86.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 219/219 [02:33<00:00,  1.43batch/s, accuracy=0.977, loss=0.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Loss: 0.0664, Accuracy: 97.67%, Time: 153.13s\n",
      "Validation Loss: 0.5176, Validation Accuracy: 87.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 219/219 [02:31<00:00,  1.45batch/s, accuracy=0.971, loss=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Loss: 0.0886, Accuracy: 97.05%, Time: 151.30s\n",
      "Validation Loss: 0.5175, Validation Accuracy: 86.09%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training phase with TQDM for batches\n",
    "    with tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as batch_tqdm:\n",
    "        for features, labels in batch_tqdm:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(features)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize model parameters\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Update batch progress bar\n",
    "            batch_tqdm.set_postfix(loss=loss.item(), accuracy=correct_predictions/total_predictions)\n",
    "\n",
    "    # Calculate training statistics\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for features, labels in val_dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(features)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_predictions += labels.size(0)\n",
    "\n",
    "    # Calculate validation accuracy and loss\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions * 100\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOWmzmzj2gre"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
